\section{Multi-way particle swarm fusion}
Multi-way Particle Swarm Fusion is a natural extension of the Fusion
Move method (FM). We call our method Swarm Fusion (SF) in short. Let us
take a multi-threading environment to explain our idea, while the
technique is also applicable to other parallel programming model such as
{\it MapReduce} in cloud computing.
%

Assuming we have $N$ threads $\{T_i | i=1, 2, \cdots, N\}$, each thread
$T_i$ maintains and updates a solution $S_i$ in parallel. Alpha
Expansion picks one label in each iteration to perform fusion. FM
generates a solution proposal via a proposal generation scheme.
%invokes one proposal generation scheme and generates a solution proposal
%for fusion.
SF has 1) a {\it proposal generator} for each thread, with which a thread
picks arbitrary number of proposal generation schemes and generates proposals, and 2) a
{\it solution pool}, from which a thread picks arbitrary number of
intermediate solutions generated by the others.
% threads.
%
% Traditional FM has a set of proposal generation schemes to be
% invoked
%
% There are two places to
% collect solutions or solution proposals for fusion. First, the {\it
% proposal pool} can generate new solution proposals on request.
% %
% Second, the {\it solution pool} stores the solutions that have been
% fused by the threads.
In our base configuration, the solution pool remembers just the best
solution from each thread.

SF has two main parameters $\alpha_i$, $\beta_i$ (for each thread
$T_i$), determining its processing flow: In each fusion step, a thread
generates $\alpha_i$ solution proposals using its proposal generator, and collects
%\chen{A proposal pool sounds confusing. We often don't really maintain such a pool as many proposals are generated dynamically based on current solution. We just have certain strategies right there and in each iteration, we pick some strategies to generate proposals on the fly. This is different with the solution pool which is really there.}
$\beta_i$ or $0$ solutions from the solution pool, based on a user-defined
strategy or at random to be simple. (The value of $\alpha_i$ and $\beta_i$ can vary in each iteration to encourage different behaviors.) The thread then fuse all these solutions
together to find a solution with lower energy state and update the solution pool accordingly.
%
% One can further customize how to pick certain proposal generation
% schemes (as in ~\cite{delong}) or solutions.


Swarm Fusion framework is very flexible and yields various data
processing architectures as shown in Figure~\ref{fig:model}.
%
\begin{figure}[tb]
 \includegraphics[width=\columnwidth]{figure/model.pdf} \caption{Swarm
   Fusion (SF) architecture and its relationships to existing methods. The
 bottom right example shows the general SF architecture, where
 each thread takes arbitrary number of solution proposals and concurrent
 solutions from other threads for fusion. The framework is flexible and
 can realize other data processing architectures depending on the
 parameters (e.g., the left two examples in the bottom row).
 %
 % The left two examples at the
 % bottom row are other Swarm fusion architectures with some restrictions,
 % for which we provide comparative evaluations in
 % Section~\ref{section:results}.
 %
% The left two examples at the bottom are other Swarm fusion
% architectures with some restrictions, to be used for evaluations later.
 %
 It is easy to verify that existing popular MRF inference methods such
 as Alpha Expansion~\cite{alpha_expansion}, Fusion Move~\cite{fusion_moves_for_markov_random_field_optimization},
 Parallel Alpha Expansion~\cite{olga_hierarchical_alpha_expansion}, or Hierarchical
 Fusion~\cite{delong_hierarchical_fusion}, are all special cases of
 SF.}
\label{fig:model}
\end{figure}
%
The bottom right architecture is the most general one, in which threads
conduct multi-way fusion of either solution proposals, or some concurrent solutions, or both (i.e.,
$\alpha\ne 0, \beta \ne 0$).
%
And the energy is not submodular
in general, for which effective inference techniques such as
TRW-S~\cite{TRW-S,opengm} exist.
%, and hence we use TRW~\cite{kolmogorov} for the fusion, while any other
%compatible inference techniques can be used~\cite{opengm}.
However, if one knows that a certain fusion step is a binary fusion with
submodular energy, one can use
graph-cuts~\cite{alpha_expansion}. If a certain fusion step is a
binary fusion with non-submodular energy, one can use
QPBO~\cite{second_order_stereo}.
%
Note that the threads appear synchronized in the figure for a
illustration purpose. In practice, all the threads run asynchronously
with a (read-write) lock on the data in the solution pool (See
Algorithm~\ref{algorithm:sf}). The asynchronous computing further unleashes the power of parallism.
%The asynchronous processing could be difficult for cloud parallel
%computing.
%
%
%
\begin{algorithm}
 \caption{Swarm Fusion method}
 \label{algorithm:sf}
 \begin{algorithmic}
  %\Procedure{Swarm Fusion method} {}
  \Procedure{} {$\alpha, \beta$}
  \State $\mathcal{S}_{pool} \leftarrow \emptyset$ //
  Solution pool
  %\State Initialize $\mathcal{P}_{pool}$ // Proposal pool
  \ForEach{thread $T_i$}
  \State Initialize its solution $S_i$
  \EndFor
  \State
  \ForEach{thread $T_i$ in parallel till convergence}
  \State Generate $\alpha$ solution proposals $\mathcal{P}$ %\subset \mathcal{P_{\mbox{pool}}}$
  \State Pick $\beta$ solutions $\mathcal{S} \subset \mathcal{S_{\mbox{pool}}}$
  \State $S_i \leftarrow \mbox{Fuse}(S_i, \mathcal{P}, \mathcal{S})$
  \State Replaces the solution in $\mathcal{S}_{pool}$ with $S_i$
  %\State [ Generates proposals and update $\mathcal{P}_{pool}$, if necessary ]
  \EndFor
  \EndProcedure
 \end{algorithmic}
\end{algorithm}


\mysubsubsection{Relationships to existing methods}

\noindent It is easy to verify that Alpha-Expansion
(AE)~\cite{alpha_expansion}, Fusion Move (FM)~\cite{fusion_moves_for_markov_random_field_optimization}, Parallel
Alpha Expansion (PAE)~\cite{fusion_moves_for_markov_random_field_optimization}, and Hierarchical Fusion
(HF)~\cite{delong_hierarchical_fusion,olga_hierarchical_alpha_expansion} are all special cases of the Swarm
Fusion method (SF). AE can be realized by setting $(\alpha=1, \beta=0)$ and
restricting the proposal generation to constant labels with a single
thread. Same goes for FM, this time, without the restriction on the
proposal generation scheme. PAE is realized by setting
$(\alpha=1,\beta=0)$ with multiple threads, again with a restriction on
the proposal generation scheme (the last sequential fusion in PAE is
realized by $(\alpha=0, \beta=1)$ with a single thread).
%
% The last sequential fusion (at the top of the example in the figure) can
% be modeled by changing to $\alpha=0, \beta=1$ with a single thread.
HF has a slightly different data processing model, without strong ties
between threads and data, but can be realized by setting ($\alpha=2,
\beta=0$) at the bottom level and ($\alpha=0, \beta=2$) at the remaining
levels, while allowing $S_i$ not to be used in the fusion steps of
$T_i$.


% The swarm fusion is highly flexible, and we first introduce our base
% architecture that exploits general strenghts offered by the
% framework. Later, we explain how to further optimize the configuration
% to boost performance for specific problems. In the base architecture,
% all the threads fuse some number of proposals from the proposal pool and
% some number of solutions from the solution pool in each step. In other
% words, all the threads constantly exchange solutions while injecting new
% proposals. This architecture is illustrated in Figure~\ref{fig:model}.


% \begin{figure}[tb]
%   \includegraphics[width=\columnwidth]{figure/model2.pdf}
%  \caption{Base Swarm Fusion architecture.}\label{fig:base}
% \end{figure}

% \mysubsubsection{Relationships to existing methods} Our framework is
% general, and it is easy to verify that existing parallel fusion
% algorithms are just our special cases.
%
% For example, the parallel fusion algorithm by Lempitsky et
% al.~\cite{viktor} always performs binary fusion between the current
% solution and another label, which can be achieved by setting $\alpha_i =
% 1$ and $\beta_i = 0$ for all the threads. The algorithm by
% Veksler~\cite{olga} or Dejong et al.~\cite{dejong} hierarchically fuses
% solutions starting from solution proposals. The algorithm can be
% realized by setting $\alpha_i = 1, \beta_i = 0$ initially, then use
% $\alpha_i = 0, \beta_i=1$  for the rest of the algorithm.

% \noindent
% We now illustrate the power of Swarm Fusion framework for three problems
% in Computer Vision. 


% Suppose $N=4$, $\alpha_1 = 2$, and $\beta_1 = 3$. In this
% configuration, the first thread $T_1$ fuses its current solution $S_1$,
% two new proposal solutions (e.g., by randomly picking two proposal
% generation schemes), and the current solutions $(S_2, S_3, S_4)$ in the
% other three threads. This is a multi-way fusion with a non-submodular
% energy in general, and message passing algorithm such as
% TRW~\cite{kolmogorov} can be used. Sequential QPBO application is
% another.


% Our model is flexible. For example, a parallel fusion move algorithm by
% Lempitsky et al.~\cite{viktor} is a special case of ours, where $\alpha_i$

% , and it is easy to see that 
% Existing parallel fusion techniques are special 

% Our model is flexible and 
